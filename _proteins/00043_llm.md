---
title: "Post 43: Leyes de escala üí∏"
collection: proteins
permalink: /proteins/00043_llm
date: 2023-08-01
---

&nbsp;

Hoy martes, martes de solo China podr√≠a haber logrado üò¨

Los modelos de lenguaje son los algoritmos que hacen que IAs como ChatGPT funcionen. Estos algoritmos aprenden m√∫ltiples temas a la vez (medicina, arquitectura, etc.). Para que esto sea posible es necesario considerar dos aspectos: la cantidad de datos usados en el entrenamiento y el n√∫mero de conexiones neuronales (AKA el tama√±o del modelo). 

Las capacidades de estos modelos siguen unas ‚Äúleyes de escala‚Äù que dicen que: a medida que aumenta el tama√±o del modelo, la IA aprende mejor los temas y, adem√°s, aparecen otros nuevos.  Como lo ejemplifican las ramas del √°rbol y su tama√±o. Por ejemplo: al duplicar el tama√±o del modelo, la IA pasa de saber biolog√≠a b√°sica a saber gen√©tica. Por esta raz√≥n, es que hay IAs cada vez m√°s grandes. Pero ¬øQu√© pasar√≠a si en vez de texto se usaran prote√≠nas?  

![img](/images/proteins/00043_llm.gif)

En 2019 aparecieron los primeros modelos de lenguaje de prote√≠nas (m√°s detalles en las refs 1 y 2). Estos modelos aprenden aspectos de las prote√≠nas como su ubicaci√≥n subcelular, tipo de plegamiento, funci√≥n, etc. ‚ÄúESM-2‚Äù, entrenada por FaceBook, era la IA de prote√≠nas m√°s grande hasta ahora y con ella predijeron la estructura de 600 millones de prote√≠nas (ref 3). Pero eso cambio hace apenas un par de semanas. 

La empresa china BioMap ha presentado su IA llamada ‚ÄúxTrimoPGLM‚Äù, la cual es ~7 veces m√°s grande que la IA de FaceBook. Adem√°s, xTrimoPGLM es la primera IA en su tipo en el mundo de las prote√≠nas, pues es tan grande como otras IAs de texto famosas como lo es GPT-3. Al compararla con otras IA, xTrimoPGLM ha conseguido un mejor desempe√±o en m√∫ltiples tareas predictivas relacionadas a las propiedades de las prote√≠nas y tambi√©n es capaz de generar secuencias. Es de esperar que nuevas estrategias deriven de esta IA en los siguientes a√±os, sin embargo, a√∫n no es seguro que BioMap haga ‚Äúopen source‚Äù este modelo y tenderemos que esperar un poco m√°s. 

Dato curioso: el entrenamiento de xTrimoPGLM empez√≥ el 18 de Enero y ‚Äútermin√≥‚Äù el 30 de Junio. El preprint se public√≥ el 14 de Julio. Lo cual significa que el equipo chino tard√≥ 14 d√≠as en hacer todos los an√°lisis presentados ggg

Refs:
1. [¬øC√≥mo pensar a las prote√≠nas?](https://miangoar.github.io/proteins/00004_think)
2. [Modelos de lenguaje de prote√≠nas](https://miangoar.github.io/proteins/00006_language)
3. [600 Millones de estructruas nuevas](https://miangoar.github.io/proteins/00026_esm)
4. [xTrimoPGLM](https://www.biorxiv.org/content/10.1101/2023.07.05.547496v3)
5. [Leyes de escala](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html )



