---
title: "Post 6: Modelos de lenguaje de prote√≠nas üíª"
collection: proteins
permalink: /proteins/00006_language
date: 2021-09-06
---

&nbsp;

[Previamente escrib√≠ c√≥mo pensar a las prote√≠nas como lenguajes](https://miangoar.github.io/proteins/00004_think). Aqu√≠, m√°s detalles de uno de estos lenguajes de prote√≠nas (ESM-1b).

Modelar la biolog√≠a es dif√≠cil, pues no suele ser aditiva, si no que tiende a ser sin√©rgica, emergente y contexto dependiente. Dado que los sistemas biol√≥gicos son m√°s que la suma de sus partes, hacer predicciones √∫tiles solo a partir de ellas es extremamente complicado. Y empeora si consideras que evolucionan en el tiempo y que lo que vemos hoy, es solo un fragmento diminuto de la biodiversidad que ha existido en la Tierra (la propiedad hist√≥rica de la biolog√≠a). Por lo que un modelo que logre capturar tanta informaci√≥n parec√≠a imposible de conseguir hasta hace poco. 

Dentro del √°rea de Inteligencia artificial, el procesamiento de lenguaje natural es una estrategia que ha permitido "semantizar" toneladas de informaci√≥n. La forma m√°s f√°cil de entender estos modelos (por ejemplo, GPT-J), es mediante la generaci√≥n de texto coherente dado una frase como base. En biolog√≠a, esto se refleja de muchas maneras, por ejemplo, aprendiendo a agrupar prote√≠nas de acuerdo con su estructura secundaria (F1).

![img](/images/proteins/00006_text.jpg)

ESM-1b (Evolutionary Scale Modeling) es un modelo que ha aprendido biolog√≠a al observar 250 millones de prote√≠nas. Su premisa es que las prote√≠nas que vemos hoy son tambi√©n "f√≥siles informacionales" que, al haber sido moldeadas por la evoluci√≥n, contienen "reglas" de lo que es m√°s biof√≠sicamente probable. ESM-1b aprendi√≥ biolog√≠a al tomar cada una de las prote√≠nas, enmascarar algunos de sus amino√°cidos y comparar est√° prote√≠na enmascarada contra su versi√≥n original para computar cu√°les son las combinaciones de amino√°cidos m√°s probables. Lo cual, le permiti√≥ "destilar" propiedades emergentes como la estructura y funci√≥n de las prote√≠nas, e incluso el grado de funcionalidad (o fitness) de las mismas (F2). 

![img](/images/proteins/00006_pipe.jpg)

ESM-1b puede usarse para predecir el fitness de las prote√≠nas y trazar trayectorias evolutivas de menos a m√°s funcionales; y luego, asignar cuales prote√≠nas fueron probablemente los ancestros de todo un conjunto. Si esto lo aplicamos al virus de la influenza, podemos ver qu√© ESM-1b identifica como puntos de origen (ancestros) a prote√≠nas que tienen origen en pandemias, como las del siglo XX o la del 2009. Permitiendo modelar parte de la din√°mica de las pandemias. ESM-1b, tambi√©n puede usarse para identificar la estructura 3D de las prote√≠nas (MSA-Transformer). Para ello se hace un entrenamiento especializado con prote√≠nas de la misma familia para aprender a distinguir que regiones de amino√°cidos son m√°s probables de tener cierta estructura secundaria y as√≠ identificar el plegamiento 3D completo. De hecho, est√° misma estrategia es usada por AlphaFold2 con un m√≥dulo llamado Evoformer (F3).

![img](/images/proteins/00006_space.jpg)

En el coraz√≥n de ESM-1b hay una operaci√≥n matem√°tica conocida como "Atenci√≥n", la cual caracteriza a un tipo de redes neuronales conocidas como ‚ÄúTransformers‚Äù. La atenci√≥n, le permite a los transformes identificar las partes m√°s relevantes de una secuencia de informaci√≥n. Si la vemos en las prote√≠nas, los amino√°cidos que tienen m√°s atenci√≥n entre si son aquellos que est√°n en contacto 3D en la prote√≠na, e incluso, hay mucha atenci√≥n a los amino√°cidos que est√°n relacionados al sitio de uni√≥n al sustrato en el caso de las enzimas. Lo cual demuestra que durante el entrenamiento de ESM-1b con millones de prote√≠nas, en cada caso, presta atenci√≥n a los amino√°cidos m√°s importantes de la estructura y funci√≥n. 

Las aplicaciones se ESM-1b y dem√°s transformes apenas empiezan. De hecho, existe otro modelo (ProGen) que ha aprendido a sintetizar prote√≠nas solo dici√©ndole el tipo de prote√≠na que quieres (p. ej. una hidrolasa de Homo sapiens de 222 amino√°cidos se longitud). Lo que nos espera en esta d√©cada dentro del √°rea del procesamiento del lenguaje natural aplicado en biolog√≠a es emocionante! Todo lo contrario a lo que nos depara el cambi√≥ clim√°tico ... 

![img](/images/proteins/00006_fold.jpg)

No quisiera ir a ning√∫n lado si mi incre√≠ble toalla.



Refs:

*Articulo base de ESM-1b*

1. [Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences](https://www.pnas.org/content/118/15/e2016239118)

*Aplicaciones a la estructura 3D de las prote√≠nas*

1. [Transformer protein language models are unsupervised structure learners](https://www.biorxiv.org/content/10.1101/2020.12.15.422761v1)
2. [BERTology Meets Biology: Interpreting Attention in Protein Language Models](https://arxiv.org/abs/2006.15222)

*Aplicaciones a la funci√≥n de 3D de las prote√≠nas*

1. [Language models enable zero-shot prediction of the effects of mutations on protein function](https://www.biorxiv.org/content/10.1101/2021.07.09.450648v1)
2. [Evolutionary velocity with protein language models](https://www.biorxiv.org/content/10.1101/2021.06.07.447389v1)
3. [Evotuning protocols for Transformer-based variant effect prediction on multi-domain proteins](https://www.biorxiv.org/content/10.1101/2021.03.05.434175v2)

*Modelo ProGen*

1. [ProGen: Language Modeling for Protein Generation](https://www.biorxiv.org/content/10.1101/2020.03.07.982272v2)
2. [Deep neural language modeling enables functional protein generation across families](https://www.biorxiv.org/content/10.1101/2021.07.18.452833v1)
